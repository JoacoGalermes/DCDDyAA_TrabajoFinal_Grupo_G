{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "WzYtTSTpBJF1",
      "metadata": {
        "id": "WzYtTSTpBJF1"
      },
      "source": [
        "![Image in a markdown cell](https://cursos.utnba.centrodeelearning.com/pluginfile.php/1/theme_space/customlogo/1738330016/Logo%20UTN%20Horizontal.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d6b6a1a-ba9f-4a43-9ad9-a545cce13c00",
      "metadata": {
        "id": "3d6b6a1a-ba9f-4a43-9ad9-a545cce13c00"
      },
      "source": [
        "# **Diplomado de Ciencia de Datos y Análisis Avanzado**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trabajo Final**\n",
        "# **Sistema de Detección de Fraude con Tarjetas de Crédito usando Machine Learning**"
      ],
      "metadata": {
        "id": "C9psa2c7689D"
      },
      "id": "C9psa2c7689D"
    },
    {
      "cell_type": "markdown",
      "id": "lafDBe3UBdM8",
      "metadata": {
        "id": "lafDBe3UBdM8"
      },
      "source": [
        "## **Curso:** Diplomado en Ciencia de Datos\n",
        "\n",
        "# **Nombres de los Miembros del Equipo:**\n",
        "### *   Arenas, Diego\n",
        "### *   Diaz, Augusto\n",
        "### *   Galermes, Joaquin\n",
        "### *   Palazón, Agustina\n",
        "### *   Telis, Monica\n",
        "### *   Vidable, Ignacio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PTmNXfGRclV0",
      "metadata": {
        "id": "PTmNXfGRclV0"
      },
      "source": [
        "## Setup del proyecto"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tx73NRa9fEPl",
      "metadata": {
        "id": "tx73NRa9fEPl"
      },
      "source": [
        "### Entorno y librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M0WFh_D7e3ol",
      "metadata": {
        "id": "M0WFh_D7e3ol"
      },
      "outputs": [],
      "source": [
        "# NOTA: Utilizamos GPU T4 como hardware accelerator para el presente trabajo.\n",
        "\n",
        "!pip install -q scikit-learn imbalanced-learn xgboost lightgbm shap matplotlib seaborn gdown\n",
        "\n",
        "import random, os, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gdown\n",
        "import sklearn, imblearn\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import shap\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import logging\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (roc_auc_score, precision_score, precision_recall_curve, PrecisionRecallDisplay,\n",
        "                             average_precision_score, classification_report,\n",
        "                             confusion_matrix, roc_curve, RocCurveDisplay, recall_score, f1_score, ConfusionMatrixDisplay)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "logging.getLogger(\"lightgbm\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"xgboost\").setLevel(logging.WARNING)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GWZGnSsAfQO1",
      "metadata": {
        "id": "GWZGnSsAfQO1"
      },
      "source": [
        "### Reproducibilidad (Seeds para reproducibilidad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yDAIA8VbfAzz",
      "metadata": {
        "id": "yDAIA8VbfAzz"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Numpy:\", np.__version__)\n",
        "print(\"Pandas:\", pd.__version__)\n",
        "print(\"Scikit-learn:\", sklearn.__version__)\n",
        "print(\"Imbalanced-learn:\", imblearn.__version__)\n",
        "print(\"XGBoost:\", xgb.__version__)\n",
        "print(\"LightGBM:\", lgb.__version__)\n",
        "print(\"SHAP:\", shap.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parámetros de costos de negocio y resolución de búsqueda de umbral"
      ],
      "metadata": {
        "id": "qBAXHNvbpqvo"
      },
      "id": "qBAXHNvbpqvo"
    },
    {
      "cell_type": "code",
      "source": [
        "costo_FN = 100.0   # Costo de dejar pasar un fraude (Falso Negativo)\n",
        "costo_FP = 1.0     # Costo de revisar una transacción legítima (Falso Positivo)\n",
        "threshold_grid = 200  # Barrido de umbrales"
      ],
      "metadata": {
        "id": "FjcsEcGopqXx"
      },
      "id": "FjcsEcGopqXx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "yrNzR8mAfZ98",
      "metadata": {
        "id": "yrNzR8mAfZ98"
      },
      "source": [
        "### Acceso a los datos y rutas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3XntOsiM_cdE",
      "metadata": {
        "id": "3XntOsiM_cdE"
      },
      "outputs": [],
      "source": [
        "# Ruta del dataset, utilizamos gdown por el peso y tamaño del archivo\n",
        "file_id = \"19gosHFxaENn4cGEm8-h_mNMOCZN8mJ7Y\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "output = \"creditcard.csv\"\n",
        "\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "df = pd.read_csv(output)\n",
        "print(\"Dataset cargado correctamente. Dimensiones:\", df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Of5FSbxjcQkn",
      "metadata": {
        "id": "Of5FSbxjcQkn"
      },
      "source": [
        "## Comprensión del Negocio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vpz4ABHTc1CQ",
      "metadata": {
        "id": "Vpz4ABHTc1CQ"
      },
      "source": [
        "### Resumen ejecutivo\n",
        "\n",
        "En este proyecto nos proponemos abordar el problema de la **detección de fraude en transacciones con tarjeta de crédito**.  \n",
        "El fraude representa una de las principales fuentes de pérdidas para bancos y empresas del sector FinTech, no solo en términos económicos directos, sino también en la confianza de los clientes y la reputación de las instituciones.  \n",
        "\n",
        "Nuestro objetivo es **desarrollar un modelo de Machine Learning** que sea capaz de identificar transacciones potencialmente fraudulentas de manera temprana y precisa.  \n",
        "Para esto, utilizamos un conjunto de datos real y anonimizado que contiene operaciones con tarjeta de crédito, donde cada registro está etiquetado como transacción legítima o fraudulenta.  \n",
        "\n",
        "El proyecto nos permitirá:  \n",
        "- Aplicar la metodología **CRISP-DM**, siguiendo las etapas de comprensión del negocio, análisis de datos, preparación, modelado y evaluación.  \n",
        "- Implementar distintos algoritmos de clasificación y comparar su desempeño mediante métricas como **AUC-ROC, Recall, Precision y F1-Score**, que son críticas en escenarios con clases desbalanceadas.  \n",
        "- Evaluar los beneficios de negocio que una solución de este tipo podría aportar, incluyendo **reducción de pérdidas económicas, mayor seguridad en las transacciones y aumento de la confianza de los clientes**.  \n",
        "\n",
        "Con este trabajo buscamos demostrar cómo la ciencia de datos puede generar valor en un caso de uso real y estratégico para el sector financiero, alineado con la creciente necesidad de **automatización y prevención de fraude en el ecosistema digital**.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Khe6qbFleJIR",
      "metadata": {
        "id": "Khe6qbFleJIR"
      },
      "source": [
        "### Definición del problema y relevancia\n",
        "\n",
        "El fraude con tarjetas de crédito es un desafío constante para las instituciones financieras y empresas del ecosistema FinTech.  \n",
        "Cada año se producen millones de transacciones fraudulentas a nivel mundial, generando pérdidas económicas significativas, además de afectar la **confianza de los clientes** y la **reputación de las entidades financieras**.  \n",
        "\n",
        "El problema que nos proponemos resolver es el siguiente:  \n",
        "**¿Podemos construir un modelo de Machine Learning que identifique de manera temprana y confiable las transacciones fraudulentas, minimizando al mismo tiempo el impacto en clientes legítimos?**  \n",
        "\n",
        "Este problema es relevante porque:  \n",
        "- Permite a las instituciones **reducir pérdidas económicas directas** al detectar intentos de fraude en tiempo real.  \n",
        "- Contribuye a **mejorar la seguridad y experiencia del cliente**, evitando bloqueos innecesarios de transacciones legítimas.  \n",
        "- Favorece la **eficiencia operativa**, al disminuir la necesidad de revisiones manuales de gran volumen de transacciones.  \n",
        "- Se alinea con los objetivos estratégicos del sector financiero: **confianza, seguridad, escalabilidad y cumplimiento regulatorio**.  \n",
        "\n",
        "**Objetivo medible:**  \n",
        "Desarrollar un modelo que alcance un **AUC-ROC superior a 0.90** y un **Recall alto (≥80%) en la clase de fraude**, asegurando la detección de la mayoría de los casos fraudulentos sin generar una cantidad excesiva de falsos positivos.  \n",
        "\n",
        "**Hipótesis de trabajo:**  \n",
        "Si aplicamos técnicas de tratamiento de desbalanceo de clases y probamos distintos algoritmos de clasificación (Logistic Regression, Random Forest, XGBoost, LightGBM), entonces podremos construir un modelo capaz de predecir fraudes con un desempeño adecuado para ser utilizado en un entorno realista del sector financiero.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xR0nU-Pwc5Jh",
      "metadata": {
        "id": "xR0nU-Pwc5Jh"
      },
      "source": [
        "## Comprensión de los Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "usqBT6MLdJwh",
      "metadata": {
        "id": "usqBT6MLdJwh"
      },
      "source": [
        "### Datos y metodología (EDA, diccionario de variables, nulos, outliers, etc.)\n",
        "\n",
        "En esta sección nos enfocamos en comprender la estructura y calidad de los datos antes de iniciar el modelado.  \n",
        "Trabajamos con el dataset **“Credit Card Fraud Detection”**, que contiene **284.807 transacciones** con información anonimizada de clientes europeos.  \n",
        "\n",
        "### Características principales del dataset (Diccionario)\n",
        "- **Time**: Tiempo en segundos desde la primera transacción en el dataset.  \n",
        "- **Amount**: Monto de la transacción en la moneda original (no especificada).  \n",
        "- **Class**: Variable objetivo (0 = legítima, 1 = fraude).  \n",
        "- **V1 – V28**: Variables anonimizadas obtenidas por reducción de dimensionalidad (PCA).  \n",
        "  - No representan un atributo único, sino combinaciones de múltiples características originales (ej. información del cliente, comercio, terminal, historial).  \n",
        "  - Aunque no podemos conocer su significado exacto por motivos de confidencialidad, es posible analizarlas estadísticamente y medir su relación con la variable objetivo.  \n",
        "  - Estudios previos indican que ciertos componentes (como V14 y V17) tienden a aportar mayor poder predictivo.  \n",
        "\n",
        "- Tamaño del dataset: 284.807 filas × 31 columnas.  \n",
        "- Distribución de clases: altamente **desbalanceada**, con aproximadamente **0,17% de transacciones fraudulentas**.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pWPcdeAOiPQ5",
      "metadata": {
        "id": "pWPcdeAOiPQ5"
      },
      "source": [
        "### Análisis descriptivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0mL44lQbiNOK",
      "metadata": {
        "id": "0mL44lQbiNOK"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6FfpTFdSiXAe",
      "metadata": {
        "id": "6FfpTFdSiXAe"
      },
      "outputs": [],
      "source": [
        "print(\"Dimensiones del dataset:\", df.shape)\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eiWTI-7iaSq",
      "metadata": {
        "id": "9eiWTI-7iaSq"
      },
      "outputs": [],
      "source": [
        "df.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6BKeogxBif0W",
      "metadata": {
        "id": "6BKeogxBif0W"
      },
      "outputs": [],
      "source": [
        "# Distribución de la variable objetivo\n",
        "fraudes = df['Class'].sum()\n",
        "total = len(df)\n",
        "print(f\"Transacciones totales: {total}\")\n",
        "print(f\"Fraudes: {fraudes} ({fraudes/total:.4%})\")\n",
        "print(f\"Legítimas: {total-fraudes} ({(total-fraudes)/total:.4%})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gVfc5cltikTF",
      "metadata": {
        "id": "gVfc5cltikTF"
      },
      "outputs": [],
      "source": [
        "# Visualizamos la distribución de la variable objetivo\n",
        "cuentas = df['Class'].value_counts().sort_index()\n",
        "porc = (cuentas / cuentas.sum() * 100).round(4)\n",
        "colores = ['#2ca02c', '#d62728']\n",
        "fig, ax = plt.subplots()\n",
        "barras = ax.bar(cuentas.index.astype(str), cuentas.values, color=colores)\n",
        "\n",
        "ax.set_yscale('log')\n",
        "ax.set_title(\"Distribución de la variable objetivo (escala log)\", pad=20)\n",
        "ax.set_xlabel(\"Clase (0 = Legítima, 1 = Fraude)\")\n",
        "ax.set_ylabel(\"Cantidad (escala log)\")\n",
        "\n",
        "# Etiquetas con conteo y porcentaje\n",
        "for i, v in enumerate(cuentas.values):\n",
        "    ax.text(i, v, f\"{v:,}\\n({porc.iloc[i]}%)\", ha='center', va='bottom')\n",
        "\n",
        "ax.legend(barras, ['Legítima (0)', 'Fraude (1)'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w5HpCEsainJl",
      "metadata": {
        "id": "w5HpCEsainJl"
      },
      "outputs": [],
      "source": [
        "# Valores faltantes\n",
        "faltantes = df.isnull().sum()\n",
        "print(\"Valores faltantes por variable:\")\n",
        "print(faltantes[faltantes > 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9jgPQi-mLnvS",
      "metadata": {
        "id": "9jgPQi-mLnvS"
      },
      "source": [
        "### Tratamiento de duplicados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4YwHT29fizmN",
      "metadata": {
        "id": "4YwHT29fizmN"
      },
      "outputs": [],
      "source": [
        "# Valores duplicados\n",
        "duplicados = df.duplicated().sum()\n",
        "print(f\"Cantidad de filas duplicadas: {duplicados}\")\n",
        "if duplicados > 0:\n",
        "    print(\"El dataset contiene filas duplicadas que podrían requerir limpieza.\")\n",
        "else:\n",
        "    print(\"No se encontraron duplicados en el dataset.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bUg_BOHSJ8ax",
      "metadata": {
        "id": "bUg_BOHSJ8ax"
      },
      "outputs": [],
      "source": [
        "# Filtramos esos duplicados para analizarlos más en detalle\n",
        "df_duplicados = df[df.duplicated(keep=False)]\n",
        "df_duplicados.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xb94qIkKLkUz",
      "metadata": {
        "id": "Xb94qIkKLkUz"
      },
      "outputs": [],
      "source": [
        "print(\"Duplicados por clase:\")\n",
        "print(df_duplicados['Class'].value_counts())\n",
        "print(\"Porcentaje de duplicados:\", (len(df_duplicados) / len(df)) * 100, \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h66Au0dQLy3w",
      "metadata": {
        "id": "h66Au0dQLy3w"
      },
      "outputs": [],
      "source": [
        "# Visualizamos si los duplicados corresponden más a fraudes o a transacciones legítimas\n",
        "sns.countplot(x=\"Class\", data=df_duplicados)\n",
        "plt.title(\"Distribución de duplicados por clase (0 = Legítima, 1 = Fraude)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8EIS3gMBRC7P",
      "metadata": {
        "id": "8EIS3gMBRC7P"
      },
      "source": [
        "### Tratamiento de outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IXKBcnE5RGoO",
      "metadata": {
        "id": "IXKBcnE5RGoO"
      },
      "source": [
        "**Nota:** En esta etapa buscamos identificar valores extremos que puedan afectar el modelado.  \n",
        "Nos enfocamos en las variables **Amount** y **Time**, ya que son las únicas con interpretación directa, mientras que las variables **V1–V28** provienen de una transformación PCA y no pueden interpretarse de manera individual.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CyLqclRRRGFD",
      "metadata": {
        "id": "CyLqclRRRGFD"
      },
      "outputs": [],
      "source": [
        "# Histograma y boxplot con escala logarítmica en el eje X\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "sns.histplot(df['Amount'], bins=100, ax=ax[0], color=\"purple\")\n",
        "ax[0].set_xscale('log')\n",
        "ax[0].set_title(\"Distribución de montos (escala log en X)\")\n",
        "ax[0].set_xlabel(\"Monto (log)\")\n",
        "\n",
        "sns.boxplot(x=df['Amount'], ax=ax[1], color=\"gold\")\n",
        "ax[1].set_xscale('log')\n",
        "ax[1].set_title(\"Boxplot de montos (escala log en X)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5EsWu_pqSWDH",
      "metadata": {
        "id": "5EsWu_pqSWDH"
      },
      "outputs": [],
      "source": [
        "# Outliers usando IQR en Amount\n",
        "Q1 = df['Amount'].quantile(0.25)\n",
        "Q3 = df['Amount'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "limite_inferior = Q1 - 1.5 * IQR\n",
        "limite_superior = Q3 + 1.5 * IQR\n",
        "\n",
        "outliers_amount = df[(df['Amount'] < limite_inferior) | (df['Amount'] > limite_superior)]\n",
        "print(f\"Cantidad de outliers en Amount: {len(outliers_amount)}\")\n",
        "print(f\"Porcentaje de outliers: {len(outliers_amount)/len(df)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bt6z9Oz2SZch",
      "metadata": {
        "id": "Bt6z9Oz2SZch"
      },
      "outputs": [],
      "source": [
        "# Distribución de outliers por clase\n",
        "print(\"Outliers por clase en Amount:\")\n",
        "print(outliers_amount['Class'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AHc-e0tGS3C9",
      "metadata": {
        "id": "AHc-e0tGS3C9"
      },
      "source": [
        "**Conclusiones:**\n",
        "- Distribución sesgada a la derecha. La gran mayoría de las transacciones son de montos bajos (cercanos a 0-200 unidades monetarias).\n",
        "- Se identificaron 31.904 outliers (≈ 11,20% del dataset) según el criterio estadístico de IQR. De esos outliers, 31.813 son transacciones legítimas y solo 91 corresponden a fraudes. Esto significa que los fraudes no se concentran en montos extremadamente altos, sino que pueden ocurrir en cualquier rango.\n",
        "- Los montos altos no parecen ser un indicador fuerte de fraude por sí solos. Eliminar outliers podría ser riesgoso, porque podríamos estar descartando información válida de transacciones legítimas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8GNPrbvuTqsF",
      "metadata": {
        "id": "8GNPrbvuTqsF"
      },
      "source": [
        "#### Análisis de la variable Time\n",
        "La variable `Time` representa los segundos transcurridos desde la primera transacción en el dataset.  \n",
        "Nuestro objetivo es visualizar su distribución general y comparar si los fraudes ocurren con mayor frecuencia en ciertas franjas horarias.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RLtQeMHsT2BD",
      "metadata": {
        "id": "RLtQeMHsT2BD"
      },
      "outputs": [],
      "source": [
        "# Histograma de Time (todas las transacciones)\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.histplot(df['Time'], bins=100, color=\"steelblue\")\n",
        "plt.title(\"Distribución de la variable Time (todas las transacciones)\")\n",
        "plt.xlabel(\"Segundos desde la primera transacción\")\n",
        "plt.ylabel(\"Cantidad\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ExCgNfkT5R6",
      "metadata": {
        "id": "7ExCgNfkT5R6"
      },
      "outputs": [],
      "source": [
        "# Creamos una nueva variable: Hora aproximada de la transacción\n",
        "df['Hour'] = (df['Time'] // 3600) % 24\n",
        "\n",
        "# Distribución de fraudes vs. no fraudes por hora\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(x='Hour', hue='Class', data=df, palette=['green','red'])\n",
        "plt.title(\"Distribución de transacciones por hora (legítimas vs fraudes)\")\n",
        "plt.xlabel(\"Hora del día\")\n",
        "plt.ylabel(\"Cantidad de transacciones\")\n",
        "plt.legend(title=\"Clase\", labels=[\"Legítima (0)\", \"Fraude (1)\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GGz6KhIyT98V",
      "metadata": {
        "id": "GGz6KhIyT98V"
      },
      "outputs": [],
      "source": [
        "# Calculamos porcentaje de fraudes por hora\n",
        "fraude_por_hora = df.groupby('Hour')['Class'].mean() * 100\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "fraude_por_hora.plot(kind='bar', color=\"crimson\")\n",
        "plt.title(\"Porcentaje de transacciones fraudulentas por hora del día\")\n",
        "plt.xlabel(\"Hora del día\")\n",
        "plt.ylabel(\"Porcentaje de fraudes (%)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HHeKtUVXUW_I",
      "metadata": {
        "id": "HHeKtUVXUW_I"
      },
      "source": [
        "**Conclusión**  \n",
        "La distribución de transacciones muestra un patrón claro de actividad durante el día, con mayor volumen entre las 8:00 y las 22:00 horas.  \n",
        "Si bien los fraudes son poco frecuentes en general, el análisis por porcentaje revela que tienen una incidencia proporcionalmente mayor en las horas de la madrugada (aproximadamente entre la 1:00 y las 4:00 AM).  \n",
        "Esto sugiere que los intentos de fraude tienden a concentrarse en horarios de baja actividad, cuando es más probable que pasen desapercibidos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M_zBNqycO8Nx",
      "metadata": {
        "id": "M_zBNqycO8Nx"
      },
      "source": [
        "#### Análisis de las variables V1-V28\n",
        "Para determinar la transformación más adecuada para las variables transformadas por PCA (V1 a V28), se generan histogramas que permitan analizar sus distribuciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y1CW_AI9PBr0",
      "metadata": {
        "id": "Y1CW_AI9PBr0"
      },
      "outputs": [],
      "source": [
        "numeric_cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
        "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
        "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28']\n",
        "\n",
        "n_cols = len(numeric_cols)\n",
        "n_rows = (n_cols + 2) // 3\n",
        "fig, axes = plt.subplots(n_rows, min(n_cols, 3), figsize=(15, 5*n_rows))\n",
        "\n",
        "axes = axes.ravel() if n_cols > 1 else [axes]\n",
        "\n",
        "# Histograma para cada variable numérica\n",
        "for i, col in enumerate(numeric_cols):\n",
        "    sns.histplot(data=df, x=col, ax=axes[i])\n",
        "    axes[i].set_title(f'Histograma de {col}')\n",
        "    axes[i].set_xlabel(col)\n",
        "    axes[i].set_ylabel('Frecuencia')\n",
        "\n",
        "for i in range(len(numeric_cols), len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lEdF6tNPPJH1",
      "metadata": {
        "id": "lEdF6tNPPJH1"
      },
      "source": [
        "**Conclusión**  \n",
        "\n",
        "Dado que las variables transformadas mediante PCA (V1 a V28) presentan distribuciones aproximadamente centradas y simétricas, según el análisis exploratorio, se opta por aplicar la estandarización Z-score Scaling."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "coxiXa8JcQad",
      "metadata": {
        "id": "coxiXa8JcQad"
      },
      "source": [
        "## Preparación de los Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q2s2uJ74VHS8",
      "metadata": {
        "id": "Q2s2uJ74VHS8"
      },
      "outputs": [],
      "source": [
        "# Copiamos el dataset original para trabajar\n",
        "df_prep = df.copy()\n",
        "\n",
        "# Transformación logarítmica de Amount (log1p para evitar log(0))\n",
        "df_prep['Amount_log'] = np.log1p(df_prep['Amount'])\n",
        "\n",
        "# Creamos la variable Hour (ya la teníamos antes pero la aseguramos aquí)\n",
        "df_prep['Hour'] = (df_prep['Time'] // 3600) % 24\n",
        "\n",
        "# Definimos las features y la variable objetivo\n",
        "X = df_prep.drop(columns=['Class', 'Time', 'Amount'])  # quitamos la original de tiempo y amount crudo\n",
        "y = df_prep['Class']\n",
        "\n",
        "print(\"Dimensiones de X:\", X.shape)\n",
        "print(\"Dimensiones de y:\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eMS-z8asVKYW",
      "metadata": {
        "id": "eMS-z8asVKYW"
      },
      "outputs": [],
      "source": [
        "# Escalamos las variables numéricas\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Escalado aplicado. Media de la primera columna:\", round(X_scaled[:,0].mean(), 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eZINvGXVMNg",
      "metadata": {
        "id": "7eZINvGXVMNg"
      },
      "outputs": [],
      "source": [
        "# Dividimos en conjuntos de entrenamiento y prueba (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, stratify=y, random_state=SEED\n",
        ")\n",
        "\n",
        "print(\"Tamaño del conjunto de entrenamiento:\", X_train.shape)\n",
        "print(\"Tamaño del conjunto de prueba:\", X_test.shape)\n",
        "print(\"Distribución en entrenamiento:\", np.bincount(y_train))\n",
        "print(\"Distribución en prueba:\", np.bincount(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YtJDklGkcQSJ",
      "metadata": {
        "id": "YtJDklGkcQSJ"
      },
      "source": [
        "## Modelado"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cUnFMhRIXujF",
      "metadata": {
        "id": "cUnFMhRIXujF"
      },
      "source": [
        "## Modelado: Baseline y Comparación de Modelos\n",
        "\n",
        "Entrenamos cinco modelos (Regresión Logística, Árbol de Decisión, Random Forest, XGBoost y LightGBM).  \n",
        "Debido al fuerte desbalance de clases, aplicamos:\n",
        "- `class_weight='balanced'` en modelos de scikit-learn.\n",
        "- `scale_pos_weight` en XGBoost y LightGBM, calculado como `negativos/positivos` en entrenamiento.\n",
        "\n",
        "Reportamos métricas en test: **AUC-ROC**, **AUC-PR (Average Precision)**, **Recall**, **Precision**, **F1** y la **Matriz de Confusión**.\n",
        "\n",
        "**Nota:** Al ejecutar los cinco modelos en Google Colab utilizando una GPU T4, el tiempo total de entrenamiento ronda los **7 minutos** aproximadamente.  \n",
        "Este tiempo es esperable dado el tamaño del dataset (~285.000 transacciones) y la complejidad de los algoritmos de ensamble como Random Forest, XGBoost y LightGBM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Tlt6tqDZYs1D",
      "metadata": {
        "id": "Tlt6tqDZYs1D"
      },
      "outputs": [],
      "source": [
        "# Nos aseguramos que X_train, X_test, y_train, y_test y SEED están definidos\n",
        "pos = y_train.sum()\n",
        "neg = len(y_train) - pos\n",
        "scale_pos_w = neg / pos if pos > 0 else 1.0\n",
        "\n",
        "print(\"Relación de clases en train -> negativos/positivos:\", round(scale_pos_w, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vNoBhuPWaFz1",
      "metadata": {
        "id": "vNoBhuPWaFz1"
      },
      "outputs": [],
      "source": [
        "# Definimos los modelos con parámetros razonables y reproducibles\n",
        "modelos = {\n",
        "    \"LogisticRegression\": LogisticRegression(\n",
        "        class_weight=\"balanced\", max_iter=200, random_state=SEED, n_jobs=-1\n",
        "    ),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(\n",
        "        class_weight=\"balanced\", random_state=SEED, max_depth=None, min_samples_leaf=1\n",
        "    ),\n",
        "    \"RandomForest\": RandomForestClassifier(\n",
        "        class_weight=\"balanced\", n_estimators=300, max_depth=None,\n",
        "        n_jobs=-1, random_state=SEED\n",
        "    ),\n",
        "    \"XGBoost\": xgb.XGBClassifier(\n",
        "        n_estimators=400, max_depth=4, learning_rate=0.1,\n",
        "        subsample=0.8, colsample_bytree=0.8,\n",
        "        scale_pos_weight=scale_pos_w,\n",
        "        eval_metric=\"auc\", n_jobs=-1, random_state=SEED, verbosity=0\n",
        "    ),\n",
        "    \"LightGBM\": lgb.LGBMClassifier(\n",
        "        n_estimators=400, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8,\n",
        "        num_leaves=31, max_depth=-1,\n",
        "        scale_pos_weight=scale_pos_w,\n",
        "        random_state=SEED, n_jobs=-1, verbosity=-1\n",
        "    )\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0wGuwnmUaYBu",
      "metadata": {
        "id": "0wGuwnmUaYBu"
      },
      "outputs": [],
      "source": [
        "# Evaluamos los modelos\n",
        "def evaluar_modelo(nombre, modelo, X_tr, y_tr, X_te, y_te):\n",
        "\n",
        "    modelo.fit(X_tr, y_tr)\n",
        "\n",
        "    y_proba = modelo.predict_proba(X_te)[:, 1] if hasattr(modelo, \"predict_proba\") else modelo.decision_function(X_te)\n",
        "    y_pred = (y_proba >= 0.5).astype(int)\n",
        "\n",
        "    auc = roc_auc_score(y_te, y_proba)\n",
        "    ap = average_precision_score(y_te, y_proba)  # AUC-PR\n",
        "    rec = recall_score(y_te, y_pred, zero_division=0)\n",
        "    prec = precision_score(y_te, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_te, y_pred, zero_division=0)\n",
        "    cm = confusion_matrix(y_te, y_pred)\n",
        "    return {\n",
        "        \"modelo\": nombre,\n",
        "        \"auc_roc\": auc,\n",
        "        \"auc_pr\": ap,\n",
        "        \"recall\": rec,\n",
        "        \"precision\": prec,\n",
        "        \"f1\": f1,\n",
        "        \"tn\": cm[0,0], \"fp\": cm[0,1], \"fn\": cm[1,0], \"tp\": cm[1,1]\n",
        "    }\n",
        "\n",
        "# Evaluamos todos y armamos tabla de resultados\n",
        "resultados = []\n",
        "for nombre, modelo in modelos.items():\n",
        "    res = evaluar_modelo(nombre, modelo, X_train, y_train, X_test, y_test)\n",
        "    resultados.append(res)\n",
        "\n",
        "tabla_resultados = pd.DataFrame(resultados).sort_values(by=[\"auc_pr\",\"auc_roc\",\"f1\"], ascending=False)\n",
        "tabla_resultados.reset_index(drop=True, inplace=True)\n",
        "tabla_resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6IbC9teoafny",
      "metadata": {
        "id": "6IbC9teoafny"
      },
      "outputs": [],
      "source": [
        "# Resumen legible del mejor modelo según AUC-PR\n",
        "mejor = tabla_resultados.iloc[0]\n",
        "print(\"Mejor modelo según AUC-PR:\")\n",
        "for c in [\"modelo\",\"auc_pr\",\"auc_roc\",\"recall\",\"precision\",\"f1\",\"tp\",\"fp\",\"fn\",\"tn\"]:\n",
        "    print(f\"{c}: {mejor[c]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "teACItJCdYSv",
      "metadata": {
        "id": "teACItJCdYSv"
      },
      "source": [
        "**Conclusion:** Tras entrenar y evaluar los modelos en el set de prueba, los mejores resultados se obtuvieron con **XGBoost**, que alcanzó un **AUC-ROC ≈ 0.98** y un **AUC-PR ≈ 0.88**, con buen equilibrio entre **Recall (≈85%)** y **Precision (≈86%)**.  \n",
        "Estos resultados lo posicionan como el modelo más adecuado para la tarea de detección de fraude en tarjetas de crédito."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "N8odr3vxqFrp"
      },
      "id": "N8odr3vxqFrp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**La siguiente matriz usa el umbral por defecto 0.5. En la sección Evaluation optimizamos el umbral por costo y reportamos las métricas finales en ese threshold.**"
      ],
      "metadata": {
        "id": "K8OvG1jGp_qU"
      },
      "id": "K8OvG1jGp_qU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2sFfqsngMoD",
      "metadata": {
        "id": "f2sFfqsngMoD"
      },
      "outputs": [],
      "source": [
        "# Reentrenamos XGBoost en train y evaluamos en test (por si acaso no quedó en memoria)\n",
        "mejor_modelo = modelos[\"XGBoost\"]\n",
        "mejor_modelo.fit(X_train, y_train)\n",
        "y_pred = mejor_modelo.predict(X_test)\n",
        "\n",
        "# Matriz de confusión\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Legítima (0)\", \"Fraude (1)\"])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "disp.plot(cmap=\"Blues\", ax=ax, values_format=\"d\")\n",
        "plt.title(\"Matriz de Confusión - XGBoost\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JRPKvJRrgtNf",
      "metadata": {
        "id": "JRPKvJRrgtNf"
      },
      "source": [
        "**Conclusión(XGBoost):**\n",
        "\n",
        "- El modelo identificó correctamente **56.851 transacciones legítimas** y solo marcó **13 falsos positivos**.  \n",
        "- Logró detectar **83 transacciones fraudulentas de un total de 98**, lo que representa una **cobertura (Recall) de aproximadamente 85%**.  \n",
        "- Solo dejó escapar **15 fraudes (falsos negativos)**, lo que es bajo en relación al total.  \n",
        "\n",
        "Esto confirma que el modelo ofrece un **muy buen balance entre detectar fraudes y no generar demasiadas alertas falsas**.  \n",
        "Desde la perspectiva de negocio, esto es clave: se logra prevenir la mayoría de los fraudes sin bloquear injustamente un número elevado de transacciones legítimas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fodFF25ucQG9",
      "metadata": {
        "id": "fodFF25ucQG9"
      },
      "source": [
        "## Evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ntB7MkJ5g_Ig",
      "metadata": {
        "id": "ntB7MkJ5g_Ig"
      },
      "source": [
        "## Evaluación e Impacto en el Negocio\n",
        "\n",
        "Los resultados obtenidos muestran que el modelo XGBoost es capaz de identificar con gran precisión los casos de fraude en tarjetas de crédito.  \n",
        "El área bajo la curva ROC se acerca a 0.98 y la curva Precision–Recall también refleja un desempeño sólido, lo que confirma que el modelo mantiene un buen equilibrio entre capturar la mayoría de fraudes y minimizar las falsas alarmas.  \n",
        "\n",
        "En la práctica, esto significa que la institución financiera podría prevenir una parte muy significativa de los fraudes realizando solo un numero reducido de revisiones manuales\n",
        "  \n",
        "Al reducir falsos positivos, se mejora la experiencia del cliente, evitando bloqueos injustificados de transacciones legítimas, y al mismo tiempo se disminuyen las pérdidas económicas asociadas al fraude.  \n",
        "\n",
        "En síntesis, la evaluación confirma que este modelo no solo es técnicamente sólido, sino que también aporta valor directo al negocio al aumentar la seguridad y confianza en las operaciones con tarjeta de crédito.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q5oY5ZRThAaA",
      "metadata": {
        "id": "q5oY5ZRThAaA"
      },
      "outputs": [],
      "source": [
        "# Probabilidades del modelo\n",
        "y_proba = mejor_modelo.predict_proba(X_test)[:,1]\n",
        "\n",
        "def costo_esperado(y_true, y_proba, threshold, c_fn=100.0, c_fp=1.0):\n",
        "    y_pred = (y_proba >= threshold).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return fn * c_fn + fp * c_fp\n",
        "\n",
        "def barrido_umbral_por_costo(y_true, y_proba, c_fn=100.0, c_fp=1.0, grid=200):\n",
        "    ths = np.linspace(0.0, 1.0, grid + 1)\n",
        "    costos = np.array([costo_esperado(y_true, y_proba, th, c_fn, c_fp) for th in ths])\n",
        "    i = costos.argmin()\n",
        "    return ths[i], float(costos[i]), ths, costos\n",
        "\n",
        "def reporte_a_umbral(y_true, y_proba, threshold):\n",
        "    y_pred = (y_proba >= threshold).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return {\n",
        "        \"threshold\": float(threshold),\n",
        "        \"TP\": int(tp), \"FP\": int(fp), \"FN\": int(fn), \"TN\": int(tn),\n",
        "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
        "        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
        "        \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
        "    }\n",
        "\n",
        "# Umbral óptimo por costo esperado\n",
        "th_opt, costo_min, ths, costos = barrido_umbral_por_costo(\n",
        "    y_test, y_proba, c_fn=costo_FN, c_fp=costo_FP, grid=threshold_grid\n",
        ")\n",
        "rep_opt = reporte_a_umbral(y_test, y_proba, th_opt)\n",
        "\n",
        "print(f\"Umbral óptimo por costo: {th_opt:.3f}  |  Costo esperado mínimo: {costo_min:.2f}\")\n",
        "print(f\"TP={rep_opt['TP']}  FP={rep_opt['FP']}  FN={rep_opt['FN']}  TN={rep_opt['TN']}\")\n",
        "print(f\"Precision={rep_opt['precision']:.4f}  Recall={rep_opt['recall']:.4f}  F1={rep_opt['f1']:.4f}\")\n",
        "\n",
        "# Curva de costo vs umbral\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(ths, costos)\n",
        "plt.axvline(th_opt, color='gray', linestyle='--')\n",
        "plt.title(\"Costo esperado vs Umbral\")\n",
        "plt.xlabel(\"Umbral\")\n",
        "plt.ylabel(\"Costo esperado\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Matriz de confusión a umbral óptimo\n",
        "y_pred_opt = (y_proba >= th_opt).astype(int)\n",
        "cm_opt = confusion_matrix(y_test, y_pred_opt)\n",
        "disp_opt = ConfusionMatrixDisplay(confusion_matrix=cm_opt, display_labels=[\"Legítima (0)\", \"Fraude (1)\"])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "disp_opt.plot(cmap=\"Greens\", ax=ax, values_format=\"d\")\n",
        "plt.title(f\"Matriz de Confusión - threshold óptimo = {th_opt:.3f}\")\n",
        "plt.show()\n",
        "\n",
        "# Curva ROC\n",
        "RocCurveDisplay.from_predictions(y_test, y_proba, name=\"XGBoost\", color=\"navy\")\n",
        "plt.title(\"Curva ROC - XGBoost\")\n",
        "plt.show()\n",
        "\n",
        "# Curva Precision-Recall\n",
        "PrecisionRecallDisplay.from_predictions(y_test, y_proba, name=\"XGBoost\", color=\"darkred\")\n",
        "plt.title(\"Curva Precision-Recall - XGBoost\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparativa: baseline 0.5 vs umbral óptimo por costo"
      ],
      "metadata": {
        "id": "yWrKEOzUrY5c"
      },
      "id": "yWrKEOzUrY5c"
    },
    {
      "cell_type": "code",
      "source": [
        "def resumen_en_umbral(y_true, y_proba, th, nombre):\n",
        "    y_pred = (y_proba >= th).astype(int)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return {\n",
        "        \"Escenario\": nombre,\n",
        "        \"Threshold\": th,\n",
        "        \"TP\": tp, \"FP\": fp, \"FN\": fn, \"TN\": tn,\n",
        "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
        "        \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n",
        "        \"F1\": f1_score(y_true, y_pred, zero_division=0),\n",
        "        \"AUC_ROC\": roc_auc_score(y_true, y_proba),\n",
        "        \"AUC_PR\": average_precision_score(y_true, y_proba)\n",
        "    }\n",
        "\n",
        "base = resumen_en_umbral(y_test, y_proba, 0.5, \"Baseline 0.5\")\n",
        "opt  = resumen_en_umbral(y_test, y_proba, th_opt, \"Óptimo por costo\")\n",
        "\n",
        "comp_df = pd.DataFrame([base, opt])\n",
        "comp_df = comp_df[[\"Escenario\",\"Threshold\",\"TP\",\"FP\",\"FN\",\"TN\",\"Precision\",\"Recall\",\"F1\",\"AUC_ROC\",\"AUC_PR\"]]\n",
        "display(comp_df.round(4))"
      ],
      "metadata": {
        "id": "iLuM04qUrYe9"
      },
      "id": "iLuM04qUrYe9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sensibilidad: cómo cambia el umbral si cambian los costos**"
      ],
      "metadata": {
        "id": "Rn7VouEsrmtr"
      },
      "id": "Rn7VouEsrmtr"
    },
    {
      "cell_type": "code",
      "source": [
        "escenarios_costos = [\n",
        "    (\"FN=50, FP=1\", 50, 1),\n",
        "    (\"FN=100, FP=1 (base)\", 100, 1),\n",
        "    (\"FN=200, FP=1\", 200, 1),\n",
        "    (\"FN=100, FP=2\", 100, 2),\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for name, cfn, cfp in escenarios_costos:\n",
        "    th, cmin, _, _ = barrido_umbral_por_costo(y_test, y_proba, c_fn=cfn, c_fp=cfp, grid=threshold_grid)\n",
        "    rep = reporte_a_umbral(y_test, y_proba, th)\n",
        "    rows.append({\n",
        "        \"EscenarioCostos\": name,\n",
        "        \"Threshold_opt\": th,\n",
        "        \"Costo_min\": cmin,\n",
        "        \"Precision\": rep[\"precision\"],\n",
        "        \"Recall\": rep[\"recall\"],\n",
        "        \"F1\": rep[\"f1\"],\n",
        "        \"TP\": rep[\"TP\"], \"FP\": rep[\"FP\"], \"FN\": rep[\"FN\"], \"TN\": rep[\"TN\"]\n",
        "    })\n",
        "\n",
        "sens_df = pd.DataFrame(rows).sort_values(\"Threshold_opt\")\n",
        "display(sens_df.round(4))"
      ],
      "metadata": {
        "id": "a0eAn3pZrilC"
      },
      "id": "a0eAn3pZrilC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "tcZYx-0DdlMP",
      "metadata": {
        "id": "tcZYx-0DdlMP"
      },
      "source": [
        "### Conclusiones finales\n",
        "\n",
        "El trabajo realizado nos permitió aplicar la metodología CRISP-DM al problema de detección de fraude en tarjetas de crédito.  \n",
        "Comenzamos con la comprensión del negocio y los datos, analizamos su distribución, detectamos la fuerte desproporción entre clases y preparamos el dataset con transformaciones adecuadas.  \n",
        "Posteriormente entrenamos y comparamos distintos modelos de clasificación, desde opciones simples como la regresión logística hasta algoritmos más avanzados de ensamble.  \n",
        "\n",
        "Los resultados muestran que el modelo **XGBoost** es el que ofrece el mejor equilibrio entre cobertura de fraudes y precisión, con métricas que lo convierten en una herramienta práctica y confiable.  \n",
        "Detecta la mayoría de los fraudes, reduce al mínimo las falsas alarmas y se ajusta a las necesidades de un contexto real en el sector financiero, donde tanto la seguridad como la experiencia del cliente son críticas.  \n",
        "\n",
        "Como recomendación, proponemos implementar este modelo como base de un sistema de detección de fraude en tiempo real.  \n",
        "Además, sugerimos continuar con un proceso de mejora continua, que incluya la actualización periódica del modelo con nuevos datos y la exploración de técnicas de optimización de hiperparámetros para potenciar aún más su desempeño.  \n",
        "De esta manera, la solución no solo se sostiene en el presente, sino que se adapta de forma dinámica a la evolución de los patrones de fraude en el futuro."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I_nEQdQndrp2",
      "metadata": {
        "id": "I_nEQdQndrp2"
      },
      "source": [
        "## Apéndice"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tJcoD9G4h92s",
      "metadata": {
        "id": "tJcoD9G4h92s"
      },
      "source": [
        "### Entorno y librerías utilizadas\n",
        "- Entorno: Google Colab, con GPU T4 habilitada.  \n",
        "- Lenguaje: Python 3.10.  \n",
        "\n",
        "### Tiempos de ejecución\n",
        "- Entrenar y evaluar los cinco modelos llevó aproximadamente **7 minutos** utilizando GPU T4.  \n",
        "- Para pruebas rápidas se podrían usar menos árboles (`n_estimators=100`) o trabajar con una muestra reducida del dataset.  \n",
        "\n",
        "### Tratamiento del desbalance de clases\n",
        "- En los modelos de scikit-learn aplicamos `class_weight=\"balanced\"`.  \n",
        "- En XGBoost y LightGBM utilizamos el parámetro `scale_pos_weight`, ajustado según la proporción de negativos/positivos en el conjunto de entrenamiento.  \n",
        "\n",
        "### Notas sobre el dataset\n",
        "- El dataset contiene **284.807 transacciones**, de las cuales solo **0,17% son fraudes**.  \n",
        "- Las variables `V1–V28` son componentes obtenidos por PCA a partir de variables originales que fueron anonimizadas por motivos de confidencialidad.  \n",
        "- Existen registros duplicados (≈1081), que se decidieron conservar ya que es posible que representen transacciones distintas con características muy similares.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IvT4oW7DirQW",
      "metadata": {
        "id": "IvT4oW7DirQW"
      },
      "source": [
        "### Interpretabilidad con SHAP\n",
        "\n",
        "Para complementar el análisis incluimos un ejercicio de interpretabilidad utilizando **SHAP (SHapley Additive exPlanations)**.  \n",
        "El objetivo es identificar qué variables tienen mayor peso en las predicciones del modelo XGBoost.  \n",
        "Aunque las variables `V1–V28` provienen de una transformación PCA y no tienen un significado directo, este análisis nos permite confirmar que el modelo no depende únicamente de una variable aislada, sino de un conjunto de factores combinados.  \n",
        "Esto aporta transparencia y robustez, aspectos importantes en el ámbito financiero.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ZfHF16CinCL",
      "metadata": {
        "id": "4ZfHF16CinCL"
      },
      "outputs": [],
      "source": [
        "# Objeto explainer sobre el modelo XGBoost\n",
        "explainer = shap.TreeExplainer(mejor_modelo)\n",
        "\n",
        "# Calculamos valores SHAP en una muestra (para acelerar el cálculo)\n",
        "X_sample = X_test[:5000]  # usamos 5000 instancias como ejemplo\n",
        "shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "# Importancia global de las variables\n",
        "plt.title(\"Importancia de Variables según SHAP - XGBoost\")\n",
        "shap.summary_plot(shap_values, X_sample, feature_names=X.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "trqwrxljjPje",
      "metadata": {
        "id": "trqwrxljjPje"
      },
      "source": [
        "**SHAP (XGBoost):**\n",
        "\n",
        "El gráfico muestra que ciertas variables anonimizadas tienen un impacto mucho mayor en la predicción de fraude que otras.  \n",
        "Entre ellas destacan **V14, V4, V12, V10 y V11**, que aparecen como las más influyentes. Esto coincide con lo reportado en estudios previos sobre este dataset, donde algunos componentes principales concentran patrones clave asociados al fraude.  \n",
        "\n",
        "Es importante notar que variables como **`Amount_log`** y **`Hour`** tienen menor peso relativo, lo que sugiere que el monto de la transacción o la hora del día, si bien aportan información, no son determinantes por sí mismos en la detección del fraude.  \n",
        "\n",
        "En síntesis, el modelo no depende de una sola variable sino de un **conjunto diverso de factores combinados**, lo cual refuerza la robustez de sus predicciones.  \n",
        "Aunque las variables `V1–V28` están anonimizadas y no tienen interpretación directa, este análisis confirma que el modelo captura patrones complejos en los datos y los utiliza de manera consistente para distinguir entre transacciones legítimas y fraudulentas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IcHoYkdDkCUU",
      "metadata": {
        "id": "IcHoYkdDkCUU"
      },
      "source": [
        "### Posibles mejoras futuras\n",
        "- Optimización de hiperparámetros mediante búsqueda en grid o bayesiana.  \n",
        "- Evaluación de técnicas de *undersampling/oversampling* más sofisticadas para el desbalance.  \n",
        "- Prueba de modelos de *deep learning* adaptados a detección de anomalías.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eWsVnsI6jgjt",
      "metadata": {
        "id": "eWsVnsI6jgjt"
      },
      "source": [
        "## **Con este análisis concluimos nuestra entrega del Trabajo Final.  Muchas gracias por su tiempo y atención.**\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}